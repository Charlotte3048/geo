import json
import re
import yaml
import argparse
import time
import math
import os
import sys
from collections import defaultdict

# ==============================================================================
# æµ·å¤–æ¦œå•åˆ†æå¼•æ“ (æ”¯æŒæ€»æ¦œå• + å­å“ç±»æ¦œå•)
# æè¿°: ä¸“é—¨ç”¨äºæµ·å¤–æ¦œå•åˆ†æï¼Œæ€»æ¦œå•æ±‡æ€»æ‰€æœ‰æ•°æ®ï¼Œå­å“ç±»æ¦œå•æŒ‰å­å“ç±»åˆ†åˆ«è®¡ç®—
# ç”¨æ³•:
# python analyze_results_oversea.py --config config/config_ha.yaml
# python analyze_results_oversea.py --config config/config_sh.yaml
# ==============================================================================

# æ·»åŠ  domestic ç›®å½•åˆ° sys.pathï¼Œä»¥ä¾¿å¯¼å…¥æƒ…æ„Ÿåˆ†ææ¨¡å—
# æ³¨æ„ï¼šæ­¤è·¯å¾„éœ€è¦æ ¹æ®å®é™…é¡¹ç›®ç»“æ„è°ƒæ•´
DOMESTIC_PATH = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'domestic')
if os.path.exists(DOMESTIC_PATH):
    sys.path.insert(0, DOMESTIC_PATH)

# å¯¼å…¥BERTæƒ…æ„Ÿåˆ†ææ¨¡å—
try:
    from sentiment.sentiment_analyzer import get_sentiment_analyzer

    USE_BERT_SENTIMENT = True
    print("âœ… BERTæƒ…æ„Ÿåˆ†ææ¨¡å—å·²å¯ç”¨")
except ImportError as e:
    USE_BERT_SENTIMENT = False
    print(f"âš ï¸  BERTæƒ…æ„Ÿåˆ†ææ¨¡å—æœªæ‰¾åˆ°ï¼Œå°†ä½¿ç”¨è§„åˆ™åŒ¹é…æ–¹å¼: {e}")


def analyze_single_answer(answer_text: str, references: list, brand_map: dict):
    """åˆ†æå•ä¸ªå›ç­”ï¼Œæå–å“ç‰Œç›¸å…³æŒ‡æ ‡"""
    raw_metrics = defaultdict(
        lambda: {
            "mentioned": 0,
            "first_pos": float('inf'),
            "is_strong": 0,
            "ref_count": 0,
            "mention_count": 0,
            "top10_points": 0,
            "sentiment_sentences": []
        }
    )
    answer_lower = answer_text.lower()

    # --- 1. æ£€æµ‹å“ç‰ŒæåŠå¹¶è®¡ç®— first_pos ---
    brand_mentions_with_pos = []
    for std_brand, aliases in brand_map.items():
        for alias in aliases:
            alias_lower = alias.lower()
            # Find all occurrences of the alias
            for match in re.finditer(re.escape(alias_lower), answer_lower):
                raw_metrics[std_brand]["mentioned"] = 1
                raw_metrics[std_brand]["mention_count"] += 1

                pos = match.start()
                if pos < raw_metrics[std_brand]["first_pos"]:
                    raw_metrics[std_brand]["first_pos"] = pos

                brand_mentions_with_pos.append({
                    "brand": std_brand,
                    "pos": pos
                })

    # --- 2. è®¡ç®— Top 10 ç§¯åˆ† (å‰10å¯è§åº¦) ---
    first_mention_positions = {}
    for mention in brand_mentions_with_pos:
        brand = mention["brand"]
        pos = mention["pos"]
        if brand not in first_mention_positions or pos < first_mention_positions[brand]:
            first_mention_positions[brand] = pos

    sorted_brands_by_pos = sorted(first_mention_positions.items(), key=lambda item: item[1])

    # ç»™å‰ 10 ä¸ªå“ç‰Œåˆ†é…ç§¯åˆ† (1st=10, 10th=1)
    for rank, (brand, pos) in enumerate(sorted_brands_by_pos):
        if rank < 10:
            points = 10 - rank  # 1st (rank 0) gets 10, 10th (rank 9) gets 1
            raw_metrics[brand]["top10_points"] = points
        else:
            break

    # --- 3. æå–åŒ…å«å“ç‰Œçš„å¥å­ï¼ˆç”¨äºBERTæƒ…æ„Ÿåˆ†æï¼‰---
    # é€‚é…ä¸­è‹±æ–‡å¥å­åˆ†å‰²
    sentences = re.split(r'[ã€‚\n.!?ï¼ï¼Ÿ]', answer_text)

    for sentence in sentences:
        sentence_lower = sentence.lower().strip()
        if not sentence_lower:
            continue

        for brand, metrics in raw_metrics.items():
            if not metrics["mentioned"]:
                continue
            # æ£€æŸ¥å¥å­ä¸­æ˜¯å¦åŒ…å«è¯¥å“ç‰Œï¼ˆæ£€æŸ¥æ‰€æœ‰åˆ«åï¼‰
            brand_aliases = brand_map.get(brand, [brand])
            for alias in brand_aliases:
                if alias.lower() in sentence_lower:
                    raw_metrics[brand]["sentiment_sentences"].append(sentence)
                    break  # é¿å…åŒä¸€å¥å­é‡å¤æ·»åŠ 

    # --- 4. æ£€æµ‹å¼ºæ¨è (is_strong) ---
    # ä¸­è‹±æ–‡å¼ºæ¨èå…³é”®è¯
    strong_patterns = [
        # ä¸­æ–‡
        r"(å¼ºçƒˆ)?æ¨è", r"é¦–é€‰", r"æœ€ä½³", r"å€¼å¾—.*?(å°è¯•|è´­ä¹°|é€‰æ‹©)",
        r"æ€§ä»·æ¯”.*?(é«˜|å¾ˆé«˜)", r"(æ˜¯|å±)?(top|best)[^ã€‚]*?(å“ç‰Œ|é€‰æ‹©|ä¹‹ä¸€)",
        r"(æˆ‘|æˆ‘ä»¬)?(æœ€|å¾ˆ)?å¸¸ä¹°", r"(ä¸ªäºº|æˆ‘)?è§‰å¾—.*?(æœ€å¥½|æœ€æ¨è)",
        # è‹±æ–‡
        r"highly\s+recommend", r"best\s+choice", r"top\s+pick", r"must\s+have",
        r"excellent", r"outstanding", r"superior", r"first\s+choice",
        r"strongly\s+recommend", r"worth\s+(buying|trying|considering)"
    ]
    negation_keywords = [
        # ä¸­æ–‡
        "ä¸æ¨è", "ä¸å¤ª", "ä¸å–œæ¬¢", "ä¸å€¼å¾—", "è¸©é›·", "é¿å‘", "æœ€å·®", "ä¸åˆé€‚", "ä¸å¦‚",
        # è‹±æ–‡
        "not recommend", "don't recommend", "wouldn't recommend", "avoid",
        "worst", "disappointing", "poor quality"
    ]

    for sentence in sentences:
        sentence_lower = sentence.lower()
        for brand, metrics in raw_metrics.items():
            if not metrics["mentioned"]:
                continue
            # æ£€æŸ¥å“ç‰Œæ˜¯å¦åœ¨å¥å­ä¸­
            brand_aliases = brand_map.get(brand, [brand])
            brand_in_sentence = any(alias.lower() in sentence_lower for alias in brand_aliases)
            if brand_in_sentence:
                if any(re.search(p, sentence_lower) for p in strong_patterns):
                    if not any(neg in sentence_lower for neg in negation_keywords):
                        raw_metrics[brand]["is_strong"] = 1

    return raw_metrics


def calculate_scores(data_list: list,
                     brand_dictionary: dict,
                     whitelist: set,
                     weights: dict,
                     analyzer=None) -> dict:
    """
    è®¡ç®—æ‰€æœ‰å“ç‰Œçš„å¾—åˆ†ï¼ˆé›†æˆBERTæƒ…æ„Ÿåˆ†æï¼‰

    è¿”å›: final_scores - å“ç‰Œå¾—åˆ†å­—å…¸
    """
    all_brands_raw_metrics = defaultdict(
        lambda: {
            "total_mentions": 0,
            "first_pos_sum": 0,
            "top10_score_sum": 0,
            "strong_recommend_count": 0,
            "total_ref_count": 0,
            "mention_in_answers": 0,
            "sentiment_sentences": []
        }
    )
    total_brand_mentions_across_all = 0

    # æ”¶é›†æ‰€æœ‰åŸå§‹æŒ‡æ ‡
    for item in data_list:
        answer = item.get("response", {}).get("answer", "")
        references = item.get("response", {}).get("references", [])

        if not answer:
            continue

        answer_metrics = analyze_single_answer(answer, references, brand_dictionary)

        for brand, metrics in answer_metrics.items():
            if brand in whitelist:
                brand_global_metrics = all_brands_raw_metrics[brand]
                brand_global_metrics["total_mentions"] += metrics["mention_count"]
                if metrics["first_pos"] != float('inf'):
                    brand_global_metrics["first_pos_sum"] += metrics["first_pos"]
                brand_global_metrics["strong_recommend_count"] += metrics["is_strong"]
                brand_global_metrics["top10_score_sum"] += metrics["top10_points"]
                brand_global_metrics["total_ref_count"] += metrics["ref_count"]
                brand_global_metrics["mention_in_answers"] += 1
                total_brand_mentions_across_all += metrics["mention_count"]

                # æ”¶é›†æƒ…æ„Ÿåˆ†æå¥å­
                brand_global_metrics["sentiment_sentences"].extend(metrics["sentiment_sentences"])

    if not all_brands_raw_metrics:
        return {}

    # åˆå§‹åŒ–æƒ…æ„Ÿåˆ†æå™¨
    sentiment_analyzer = analyzer
    if sentiment_analyzer is None and USE_BERT_SENTIMENT:
        try:
            sentiment_analyzer = get_sentiment_analyzer()
            print("ğŸ¤– ä½¿ç”¨BERTæ¨¡å‹è¿›è¡Œæƒ…æ„Ÿåˆ†æ...")
        except Exception as e:
            print(f"âš ï¸  BERTæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°è§„åˆ™åŒ¹é…: {e}")
            sentiment_analyzer = None

    # è®¡ç®—å½’ä¸€åŒ–å‚æ•°
    max_mentions = max((m["total_mentions"] for m in all_brands_raw_metrics.values()), default=1)
    max_strong = max((m["strong_recommend_count"] for m in all_brands_raw_metrics.values()), default=1)

    # è®¡ç®—æ¯ä¸ªå“ç‰Œçš„å¾—åˆ†
    final_scores = {}
    for brand, metrics in all_brands_raw_metrics.items():
        avg_pos = metrics["first_pos_sum"] / metrics["mention_in_answers"] if metrics[
                                                                                  "mention_in_answers"] > 0 else float(
            'inf')
        mention_density = metrics["total_mentions"] / metrics["mention_in_answers"] if metrics[
                                                                                           "mention_in_answers"] > 0 else 0

        # 1. å“ç‰Œå›ç­”æ˜¾è‘—åº¦ (Brand Prominence)
        if avg_pos == float('inf'):
            score_visibility = 0
        elif avg_pos < 500:
            score_visibility = 100
        elif avg_pos < 1500:
            score_visibility = 100 * (1 - (avg_pos - 500) / 1000)
        else:
            score_visibility = 0

        # 2. å£°é‡å æ¯” (Share of Voice)
        share_of_voice_ratio = metrics[
                                   "total_mentions"] / total_brand_mentions_across_all if total_brand_mentions_across_all > 0 else 0
        share_of_voice = (math.log(share_of_voice_ratio * 1000 + 1) / math.log(1001)) * 100

        # 3. å‰10å¯è§åº¦ (Top10 Visibility)
        max_top10_score = max((m["top10_score_sum"] for m in all_brands_raw_metrics.values()), default=1)
        normalized_top10 = (metrics["top10_score_sum"] + 1) / (max_top10_score + 1)
        top10_visibility = math.sqrt(normalized_top10) * 100

        # 4. ç«äº‰åŠ›æŒ‡æ•° (Competitiveness)
        competitiveness = (metrics["total_mentions"] / max_mentions) * 100 if max_mentions > 0 else 0

        # 5. æƒ…æ„Ÿåˆ†æ (Sentiment Analysis)
        if sentiment_analyzer and metrics["sentiment_sentences"]:
            sentences_to_analyze = metrics["sentiment_sentences"][:50]
            results = sentiment_analyzer.predict(sentences_to_analyze)
            sentiment_scores = [r["score"] for r in results]
            sentiment_analysis = sum(sentiment_scores) / len(sentiment_scores) if sentiment_scores else 50.0
        else:
            normalized_strong = (metrics["strong_recommend_count"] + 1) / (max_strong + 1)
            sentiment_analysis = math.sqrt(normalized_strong) * 100

        # åŠ æƒå¹³å‡
        w_brand_prominence = weights.get("brand_prominence", 20)
        w_share_of_voice = weights.get("share_of_voice", 20)
        w_top10_visibility = weights.get("top10_visibility", 20)
        w_competitiveness = weights.get("competitiveness", 20)
        w_sentiment = weights.get("sentiment_analysis", 20)

        total_score = (
                              score_visibility * w_brand_prominence +
                              share_of_voice * w_share_of_voice +
                              top10_visibility * w_top10_visibility +
                              competitiveness * w_competitiveness +
                              sentiment_analysis * w_sentiment
                      ) / 100

        final_scores[brand] = {
            "å“ç‰ŒæŒ‡æ•°": total_score,
            "æ€»æåŠæ¬¡æ•°": metrics["total_mentions"],
            "å‡ºç°æ¬¡æ•°": metrics["mention_in_answers"],
            "å¼ºæ¨èæ¬¡æ•°": metrics["strong_recommend_count"],
            "å¹³å‡æåŠå¯†åº¦": mention_density,
            "ç»´åº¦å¾—åˆ†": {
                "brand_prominence": score_visibility,
                "share_of_voice": share_of_voice,
                "top10_visibility": top10_visibility,
                "competitiveness": competitiveness,
                "sentiment_analysis": sentiment_analysis
            }
        }

    return final_scores


def write_ranking_report(output_file: str, title: str, total_scores: dict, task_name: str,
                         subcategory_scores: dict = None):
    """
    ç”ŸæˆMarkdownæ ¼å¼çš„æ’åæŠ¥å‘Šï¼ˆåŒ…å«æ€»æ¦œå•å’Œå­å“ç±»æ¦œå•ï¼‰

    å‚æ•°:
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        title: æŠ¥å‘Šæ ‡é¢˜
        total_scores: æ€»æ¦œå•å¾—åˆ†
        task_name: ä»»åŠ¡åç§°
        subcategory_scores: å­å“ç±»æ¦œå•å¾—åˆ†å­—å…¸ {å­å“ç±»å: å¾—åˆ†å­—å…¸}
    """
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(f"{title}\n\n")
        f.write(f"**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"**åˆ†æä»»åŠ¡**: {task_name}\n\n")
        f.write("---\n\n")

        if not total_scores:
            f.write("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•å“ç‰Œå¾—åˆ†æ•°æ®ã€‚\n\n")
            return

        # ==================== æ€»æ¦œå• ====================
        f.write("## ğŸ“Š å“ç‰Œæ’åæ€»æ¦œå•\n\n")
        f.write(
            "| æ’å | å“ç‰Œåç§° | å“ç‰ŒæŒ‡æ•° | æ€»æåŠæ¬¡æ•° | å‡ºç°æ¬¡æ•° | å¼ºæ¨èæ¬¡æ•° | å“ç‰Œæ˜¾è‘—åº¦ | å£°é‡å æ¯” | å‰10å¯è§åº¦ | ç«äº‰åŠ›æŒ‡æ•° | æƒ…æ„Ÿåˆ†æ |\n")
        f.write("|:---:|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n")

        sorted_brands = sorted(total_scores.items(), key=lambda x: x[1]["å“ç‰ŒæŒ‡æ•°"], reverse=True)

        for rank, (brand, data) in enumerate(sorted_brands, 1):
            dims = data["ç»´åº¦å¾—åˆ†"]
            row_content = (
                f"| {rank} | {brand} | **{data['å“ç‰ŒæŒ‡æ•°']:.2f}** | "
                f"{data['æ€»æåŠæ¬¡æ•°']} | {data['å‡ºç°æ¬¡æ•°']} | {data['å¼ºæ¨èæ¬¡æ•°']} | "
                f"{dims['brand_prominence']:.1f} | {dims['share_of_voice']:.1f} | {dims['top10_visibility']:.1f} | "
                f"{dims['competitiveness']:.1f} | {dims['sentiment_analysis']:.1f} |"
            )
            f.write(row_content + "\n")

        f.write("\n")

        # ==================== å­å“ç±»æ¦œå• ====================
        if subcategory_scores:
            f.write("---\n\n")
            f.write("## ğŸ“‚ å­å“ç±»æ¦œå•\n\n")
            f.write("> ä»¥ä¸‹æ¦œå•æŒ‰å­å“ç±»åˆ†åˆ«è®¡ç®—ï¼Œæ¯ä¸ªå­å“ç±»çš„å“ç‰ŒæŒ‡æ•°åŸºäºè¯¥å­å“ç±»ä¸‹çš„æ•°æ®ç‹¬ç«‹è®¡ç®—ã€‚\n\n")

            for subcategory in sorted(subcategory_scores.keys()):
                scores = subcategory_scores[subcategory]

                if not scores:
                    continue

                f.write(f"### ğŸ“Œ {subcategory}\n\n")
                # ç®€åŒ–çš„è¡¨æ ¼ï¼šåªæ˜¾ç¤ºæ’åã€å“ç‰Œåç§°ã€å“ç‰ŒæŒ‡æ•°ã€æ€»æåŠæ¬¡æ•°ã€å‡ºç°æ¬¡æ•°
                f.write("| æ’å | å“ç‰Œåç§° | å“ç‰ŒæŒ‡æ•° | æ€»æåŠæ¬¡æ•° | å‡ºç°æ¬¡æ•° |\n")
                f.write("|:---:|:---|:---:|:---:|:---:|\n")

                # æŒ‰å“ç‰ŒæŒ‡æ•°æ’åº
                sorted_subcategory_brands = sorted(scores.items(), key=lambda x: x[1]["å“ç‰ŒæŒ‡æ•°"], reverse=True)

                for rank, (brand, data) in enumerate(sorted_subcategory_brands, 1):
                    row_content = (
                        f"| {rank} | {brand} | **{data['å“ç‰ŒæŒ‡æ•°']:.2f}** | "
                        f"{data['æ€»æåŠæ¬¡æ•°']} | {data['å‡ºç°æ¬¡æ•°']} |"
                    )
                    f.write(row_content + "\n")

                f.write("\n")

        # ==================== ç»Ÿè®¡ä¿¡æ¯ ====================
        f.write("---\n\n")
        f.write("## ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯\n\n")
        f.write(f"- **å‚ä¸æ’åå“ç‰Œæ•°**: {len(total_scores)}\n")
        f.write(f"- **æœ€é«˜å“ç‰ŒæŒ‡æ•°**: {sorted_brands[0][1]['å“ç‰ŒæŒ‡æ•°']:.2f} ({sorted_brands[0][0]})\n")
        f.write(f"- **å¹³å‡å“ç‰ŒæŒ‡æ•°**: {sum(s['å“ç‰ŒæŒ‡æ•°'] for s in total_scores.values()) / len(total_scores):.2f}\n")
        f.write(f"- **æ€»æåŠæ¬¡æ•°**: {sum(s['æ€»æåŠæ¬¡æ•°'] for s in total_scores.values())}\n")
        if subcategory_scores:
            f.write(f"- **å­å“ç±»æ•°é‡**: {len(subcategory_scores)}\n")
        f.write("\n")

        # ==================== è¯„åˆ†è¯´æ˜ ====================
        f.write("## ğŸ“ è¯„åˆ†è¯´æ˜\n\n")
        f.write("æœ¬æ¦œå•é‡‡ç”¨äº”ç»´åº¦è¯„åˆ†ä½“ç³»ï¼Œæ¯ä¸ªæŒ‡æ ‡å„å 20%æƒé‡ï¼š\n\n")
        f.write("1. **å“ç‰Œæ˜¾è‘—åº¦ (Brand Prominence)**: å“ç‰Œåœ¨AIå›ç­”ä¸­å‡ºç°çš„ä½ç½®ï¼Œä½ç½®è¶Šé å‰ï¼Œåˆ†æ•°è¶Šé«˜\n")
        f.write("2. **å£°é‡å æ¯” (Share of Voice)**: å“ç‰ŒæåŠæ¬¡æ•°ä¸æ‰€æœ‰å“ç‰ŒæåŠæ¬¡æ•°çš„æ¯”ç‡\n")
        f.write("3. **å‰10å¯è§åº¦ (Top10 Visibility)**: å“ç‰Œåœ¨AIå›ç­”ä¸­å‰ååå‡ºç°çš„æ•ˆæœï¼Œåæ¬¡è¶Šé«˜åˆ†æ•°è¶Šé«˜\n")
        f.write("4. **ç«äº‰åŠ›æŒ‡æ•° (Competitiveness)**: å“ç‰Œä¸æåŠç‡æœ€é«˜å“ç‰Œçš„æåŠç‡ä¹‹æ¯”\n")
        f.write("5. **æƒ…æ„Ÿåˆ†æ (Sentiment Analysis)**: AIå›ç­”ä¸­å…³äºå“ç‰Œçš„æ­£/è´Ÿé¢å†…å®¹åˆ†æ\n")
        f.write("\n")


def main():
    parser = argparse.ArgumentParser(description="æµ·å¤–æ¦œå•åˆ†æå¼•æ“")
    parser.add_argument("--config", required=True, help="é…ç½®æ–‡ä»¶è·¯å¾„ (ä¾‹å¦‚: config_home_appliance.yaml)")
    args = parser.parse_args()

    print(f"\n{'=' * 60}")
    print(f"æµ·å¤–æ¦œå•åˆ†æå¼•æ“")
    print(f"{'=' * 60}\n")

    # åŠ è½½é…ç½®æ–‡ä»¶
    print(f"ğŸ“‹ åŠ è½½é…ç½®æ–‡ä»¶: {args.config}")
    try:
        with open(args.config, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: é…ç½®æ–‡ä»¶ '{args.config}' æœªæ‰¾åˆ°ã€‚")
        return
    except Exception as e:
        print(f"âŒ é”™è¯¯: åŠ è½½é…ç½®æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return

    # è§£æé…ç½®
    task_name = config.get("task_name", "unknown")
    results_file = config.get("results_file", "")
    output_file = config.get("ranking_output_file", f"ranking_report_{task_name}.md")
    report_title = config.get("report_title", f"# {task_name.upper()} å“ç‰ŒAIè®¤çŸ¥æŒ‡æ•°æ’è¡Œæ¦œ")
    weights = config.get("weights", {
        "brand_prominence": 20,
        "share_of_voice": 20,
        "top10_visibility": 20,
        "competitiveness": 20,
        "sentiment_analysis": 20
    })
    brand_dictionary = config.get("brand_dictionary", {})
    brands_whitelist = set(config.get("brands_whitelist", []))

    print(f"ğŸ“ ä»»åŠ¡åç§°: {task_name}")
    print(f"ğŸ“ ç»“æœæ–‡ä»¶: {results_file}")
    print(f"ğŸ“„ è¾“å‡ºæŠ¥å‘Š: {output_file}")
    print(f"ğŸ“– å“ç‰Œè¯å…¸: {len(brand_dictionary)} ä¸ªå“ç‰Œ")
    print(f"ğŸ“‹ ç™½åå•: {len(brands_whitelist)} ä¸ªå“ç‰Œ\n")

    # åŠ è½½ç»“æœæ•°æ®
    print("æ­£åœ¨åŠ è½½ç»“æœæ•°æ®...")
    try:
        with open(results_file, 'r', encoding='utf-8') as f:
            data_list = json.load(f)
        print(f"âœ… æˆåŠŸåŠ è½½ {len(data_list)} æ¡å›ç­”è®°å½•\n")
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: ç»“æœæ–‡ä»¶ '{results_file}' æœªæ‰¾åˆ°ã€‚")
        return
    except Exception as e:
        print(f"âŒ é”™è¯¯: åŠ è½½ç»“æœæ•°æ®æ—¶å‡ºé”™: {e}")
        return

    # æŒ‰å­å“ç±»åˆ†ç»„æ•°æ®
    subcategory_data = defaultdict(list)
    for item in data_list:
        category = item.get("category", "")
        if "-" in category:
            parts = category.split("-", 1)
            if len(parts) > 1:
                subcategory = parts[1]
                subcategory_data[subcategory].append(item)

    print(f"ğŸ“‚ å‘ç° {len(subcategory_data)} ä¸ªå­å“ç±»: {', '.join(sorted(subcategory_data.keys()))}\n")

    # ==================== è®¡ç®—æ€»æ¦œå• ====================
    print("æ­£åœ¨è®¡ç®—æ€»æ¦œå•...")
    total_scores = calculate_scores(data_list, brand_dictionary, brands_whitelist, weights)
    print(f"âœ… æ€»æ¦œå•: æˆåŠŸè®¡ç®— {len(total_scores)} ä¸ªå“ç‰Œçš„å¾—åˆ†\n")

    # ==================== è®¡ç®—å­å“ç±»æ¦œå• ====================
    subcategory_scores = {}
    if subcategory_data:
        print("æ­£åœ¨è®¡ç®—å­å“ç±»æ¦œå•...")
        for subcategory, sub_data in sorted(subcategory_data.items()):
            print(f"  - æ­£åœ¨å¤„ç†å­å“ç±»: {subcategory} ({len(sub_data)} æ¡è®°å½•)")
            scores = calculate_scores(sub_data, brand_dictionary, brands_whitelist, weights)
            subcategory_scores[subcategory] = scores
            print(f"    âœ… è®¡ç®—äº† {len(scores)} ä¸ªå“ç‰Œçš„å¾—åˆ†")
        print()

    # ç”ŸæˆæŠ¥å‘Š
    print("æ­£åœ¨ç”Ÿæˆæ’åæŠ¥å‘Š...")
    write_ranking_report(
        output_file,
        report_title,
        total_scores,
        task_name,
        subcategory_scores
    )
    print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}\n")

    # æ˜¾ç¤ºå‰10å
    if total_scores:
        print("ğŸ† Top 10 å“ç‰Œé¢„è§ˆ (æ€»æ¦œå•):")
        print("-" * 60)
        sorted_brands = sorted(total_scores.items(), key=lambda x: x[1]["å“ç‰ŒæŒ‡æ•°"], reverse=True)
        for rank, (brand, data) in enumerate(sorted_brands[:10], 1):
            print(f"  {rank:2d}. {brand:15s} - å“ç‰ŒæŒ‡æ•°: {data['å“ç‰ŒæŒ‡æ•°']:6.2f}")
        print("-" * 60)

    print(f"\n{'=' * 60}")
    print("åˆ†æå®Œæˆï¼")
    print(f"{'=' * 60}\n")


if __name__ == "__main__":
    main()