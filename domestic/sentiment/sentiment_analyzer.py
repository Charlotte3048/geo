# domestic/sentiment_analyzer.py
"""
BERTæƒ…æ„Ÿåˆ†ææ¨¡å—
ç”¨äºå¯¹å“ç‰Œç›¸å…³å¥å­è¿›è¡Œäº”çº§æƒ…æ„Ÿåˆ†æ
"""
import os
import sys
import torch
import numpy as np
from typing import List, Dict
from pathlib import Path
import warnings

# å¿½ç•¥ä¸€äº›è­¦å‘Š
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
BASE_DIR = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(BASE_DIR))

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œé¿å…æŸäº›åº“çš„å…¼å®¹æ€§é—®é¢˜
os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'

try:
    from transformers import AutoTokenizer, AutoModelForSequenceClassification
    from peft import PeftModel
except ImportError as e:
    print(f"âŒ ä¾èµ–åº“å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·è¿è¡Œ: pip install transformers peft torch")
    raise

# ======================================================
# é…ç½®
# ======================================================
BASE_MODEL_NAME = "bert-base-uncased"
PROJECT_ROOT = Path(__file__).resolve().parents[2]
LORA_ADAPTER_PATH = PROJECT_ROOT / "ml" / "artifacts" / "lora_adapter_v1"
MAX_LENGTH = 256
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# æƒ…æ„Ÿæ ‡ç­¾æ˜ å°„
ID2LABEL = {
    0: "strong_negative",
    1: "negative",
    2: "neutral",
    3: "positive",
    4: "strong_positive"
}

# æƒ…æ„Ÿåˆ†æ•°æ˜ å°„ (0-100åˆ†åˆ¶)
SENTIMENT_SCORES = {
    "strong_negative": 0,
    "negative": 25,
    "neutral": 50,
    "positive": 75,
    "strong_positive": 100
}


# ======================================================
# æ¨¡å‹åŠ è½½ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
# ======================================================
class SentimentAnalyzer:
    _instance = None

    @classmethod
    def get_instance(cls):
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(SentimentAnalyzer, cls).__new__(cls)
            cls._instance._load_model()
        return cls._instance

    def _load_model(self):
        print(f"ğŸ”„ æ­£åœ¨åŠ è½½BERTæƒ…æ„Ÿåˆ†ææ¨¡å‹...")
        print(f"   è®¾å¤‡: {DEVICE}")
        print(f"   Base model: {BASE_MODEL_NAME}")
        print(f"   Adapterè·¯å¾„: {LORA_ADAPTER_PATH}")

        try:
            # 1ï¸âƒ£ tokenizer ä¸€å®šæ¥è‡ª base model
            self._tokenizer = AutoTokenizer.from_pretrained(
                BASE_MODEL_NAME,
                local_files_only=False
            )

            # 2ï¸âƒ£ åŠ è½½ base model
            base_model = AutoModelForSequenceClassification.from_pretrained(
                BASE_MODEL_NAME,
                num_labels=len(ID2LABEL),
                torch_dtype=torch.float32
            )

            # 3ï¸âƒ£ åŠ è½½ LoRA adapterï¼ˆæœ¬åœ°è·¯å¾„æ˜¯å®Œå…¨ OK çš„ï¼‰
            self._model = PeftModel.from_pretrained(
                base_model,
                str(LORA_ADAPTER_PATH),
                torch_dtype=torch.float32
            )

            self._model.to(DEVICE)
            self._model.eval()

            print("âœ… BERTæ¨¡å‹åŠ è½½æˆåŠŸ\n")

        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise

    @staticmethod
    def _softmax(logits: np.ndarray) -> np.ndarray:
        """è®¡ç®—softmaxæ¦‚ç‡"""
        exp = np.exp(logits - np.max(logits, axis=-1, keepdims=True))
        return exp / exp.sum(axis=-1, keepdims=True)

    @torch.no_grad()
    def predict(self, texts: List[str], return_probs: bool = False) -> List[Dict]:
        """
        å¯¹æ–‡æœ¬åˆ—è¡¨è¿›è¡Œæƒ…æ„Ÿåˆ†æ

        Args:
            texts: å¾…åˆ†æçš„æ–‡æœ¬åˆ—è¡¨
            return_probs: æ˜¯å¦è¿”å›æ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡åˆ†å¸ƒ

        Returns:
            åŒ…å«æƒ…æ„Ÿæ ‡ç­¾ã€ç½®ä¿¡åº¦å’Œåˆ†æ•°çš„å­—å…¸åˆ—è¡¨
        """
        if not texts:
            return []

        try:
            # Tokenize
            inputs = self._tokenizer(
                texts,
                truncation=True,
                padding=True,
                max_length=MAX_LENGTH,
                return_tensors="pt"
            ).to(DEVICE)

            # æ¨ç†
            outputs = self._model(**inputs)
            logits = outputs.logits.cpu().numpy()
            probs = self._softmax(logits)
            preds = probs.argmax(axis=1)

            # æ„å»ºç»“æœ
            results = []
            for i, idx in enumerate(preds):
                label = ID2LABEL[int(idx)]
                result = {
                    "label": label,
                    "confidence": float(probs[i][idx]),
                    "score": SENTIMENT_SCORES[label]  # 0-100åˆ†åˆ¶
                }
                if return_probs:
                    result["probs"] = {
                        ID2LABEL[j]: float(probs[i][j])
                        for j in range(len(ID2LABEL))
                    }
                results.append(result)

            return results

        except Exception as e:
            print(f"âš ï¸  æ¨ç†è¿‡ç¨‹å‡ºé”™: {e}")
            # è¿”å›é»˜è®¤ä¸­æ€§ç»“æœ
            return [{
                "label": "neutral",
                "confidence": 0.0,
                "score": 50
            } for _ in texts]

    def analyze_sentence(self, sentence: str) -> Dict:
        """
        åˆ†æå•ä¸ªå¥å­çš„æƒ…æ„Ÿ

        Args:
            sentence: å¾…åˆ†æçš„å¥å­

        Returns:
            åŒ…å«æƒ…æ„Ÿæ ‡ç­¾ã€ç½®ä¿¡åº¦å’Œåˆ†æ•°çš„å­—å…¸
        """
        results = self.predict([sentence])
        return results[0] if results else {
            "label": "neutral",
            "confidence": 0.0,
            "score": 50
        }


# ======================================================
# å…¨å±€å•ä¾‹
# ======================================================
_analyzer = None


def get_sentiment_analyzer():
    """è·å–æƒ…æ„Ÿåˆ†æå™¨å•ä¾‹"""
    global _analyzer
    if _analyzer is None:
        _analyzer = SentimentAnalyzer()
    return _analyzer


# ======================================================
# ä¾¿æ·å‡½æ•°
# ======================================================
def analyze_brand_sentiment(sentences: List[str]) -> float:
    """
    åˆ†æå“ç‰Œç›¸å…³å¥å­çš„å¹³å‡æƒ…æ„Ÿå¾—åˆ†

    Args:
        sentences: åŒ…å«å“ç‰Œçš„å¥å­åˆ—è¡¨

    Returns:
        å¹³å‡æƒ…æ„Ÿå¾—åˆ† (0-100)
    """
    if not sentences:
        return 50.0  # é»˜è®¤ä¸­æ€§

    try:
        analyzer = get_sentiment_analyzer()
        results = analyzer.predict(sentences)

        # è®¡ç®—å¹³å‡åˆ†æ•°
        scores = [r["score"] for r in results]
        avg_score = sum(scores) / len(scores) if scores else 50.0

        return avg_score
    except Exception as e:
        print(f"âš ï¸  æƒ…æ„Ÿåˆ†æå¤±è´¥: {e}")
        return 50.0  # è¿”å›é»˜è®¤ä¸­æ€§åˆ†æ•°


# ======================================================
# æµ‹è¯•ä»£ç 
# ======================================================
if __name__ == "__main__":
    # æµ‹è¯•æ ·ä¾‹
    test_sentences = [
        "This brand is absolutely amazing and I highly recommend it!",
        "The product quality is terrible and customer service is even worse.",
        "It's okay, nothing special but not bad either.",
        "Best purchase I've ever made! Will definitely buy again.",
        "Completely disappointed with this brand."
    ]

    print("\n" + "=" * 70)
    print("æƒ…æ„Ÿåˆ†ææ¨¡å—æµ‹è¯•")
    print("=" * 70 + "\n")

    try:
        analyzer = get_sentiment_analyzer()
        results = analyzer.predict(test_sentences, return_probs=True)

        print("æµ‹è¯•ç»“æœ:")
        print("-" * 70)

        for text, result in zip(test_sentences, results):
            print(f"\næ–‡æœ¬: {text}")
            print(f"æƒ…æ„Ÿ: {result['label']}")
            print(f"ç½®ä¿¡åº¦: {result['confidence']:.3f}")
            print(f"å¾—åˆ†: {result['score']}/100")
            if "probs" in result:
                print("æ¦‚ç‡åˆ†å¸ƒ:")
                for label, prob in result["probs"].items():
                    print(f"  {label:20s}: {prob:.3f}")

        print("\n" + "=" * 70)
        print("âœ… æµ‹è¯•å®Œæˆ")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
