import streamlit as st
import pandas as pd
import time
from collections import defaultdict

# --- å¤ç”¨ analyze_results.py ä¸­çš„æ ¸å¿ƒä»£ç  ---
# ä¸ºäº†è®© dashboard.py èƒ½ç‹¬ç«‹è¿è¡Œï¼Œæˆ‘ä»¬å°†åˆ†æé€»è¾‘ç›´æ¥æ•´åˆè¿›æ¥ã€‚
# (åœ¨æ­¤å¤„ç²˜è´´ analyze_results.py ä¸­çš„å‡½æ•°å’Œé…ç½®)

import json
import os
import re
import math

# --- 1. é…ç½® (ä» analyze_results.py å¤åˆ¶) ---
RESULTS_FILE = "results.json"
BRAND_DICTIONARY = {
    # ... (åœ¨æ­¤å¤„ç²˜è´´æ‚¨å·²æœ‰çš„å®Œæ•´è¯å…¸)
}
CHINESE_BRANDS_WHITELIST = {
    # ... (åœ¨æ­¤å¤„ç²˜è´´æ‚¨å·²æœ‰çš„å®Œæ•´ç™½åå•)
}
WEIGHTS_V3 = {
    "visibility": 20, "mention_rate": 20, "ai_ranking": 20,
    "ref_depth": 15, "mind_share": 15, "competitiveness": 10,
}


# --- 2. æ ¸å¿ƒåˆ†æå‡½æ•° (ä» analyze_results.py å¤åˆ¶) ---
def normalize_score(value, max_value, min_value=0, scale=100):
    if max_value == min_value: return scale if value > 0 else 0
    log_value = math.log1p(value)
    log_max = math.log1p(max_value)
    return (log_value / log_max) * scale if log_max > 0 else 0


def analyze_single_answer(answer_text: str, references: list, brand_map: dict):
    raw_metrics = defaultdict(
        lambda: {"mentioned": 0, "first_pos": float('inf'), "is_strong": 0, "ref_count": 0, "mention_count": 0})
    answer_lower = answer_text.lower()
    for std_brand, aliases in brand_map.items():
        for alias in aliases:
            alias_lower = alias.lower()
            if alias_lower in answer_lower:
                raw_metrics[std_brand]["mentioned"] = 1
                raw_metrics[std_brand]["mention_count"] += answer_lower.count(alias_lower)
                try:
                    pos = answer_lower.index(alias_lower)
                    if pos < raw_metrics[std_brand]["first_pos"]: raw_metrics[std_brand]["first_pos"] = pos
                except ValueError:
                    pass
    sentences = re.split(r'[ã€‚\n]', answer_text)
    for sentence in sentences:
        sentence_lower = sentence.lower()
        for brand, metrics in raw_metrics.items():
            if metrics["mentioned"] and brand.lower() in sentence_lower:
                if any(word in sentence_lower for word in
                       ["é¦–é€‰", "æœ€ä½³", "å¼ºçƒˆæ¨è", "best", "top pick", "most recommended"]): raw_metrics[brand][
                    "is_strong"] = 1
    if references:
        for brand, metrics in raw_metrics.items():
            if metrics["mentioned"]:
                for ref in references:
                    if brand.lower() in ref.lower(): raw_metrics[brand]["ref_count"] += 1
    return raw_metrics


# --- 3. æ•°æ®å¤„ç†ä¸ç¼“å­˜ ---
@st.cache_data(ttl=600)  # ç¼“å­˜æ•°æ®10åˆ†é’Ÿï¼Œé¿å…æ¯æ¬¡åˆ·æ–°éƒ½é‡æ–°è®¡ç®—
def get_ranking_data():
    """æ‰§è¡Œå®Œæ•´çš„åˆ†ææµç¨‹å¹¶è¿”å›æ ¼å¼åŒ–çš„æ•°æ®"""
    try:
        with open(RESULTS_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except FileNotFoundError:
        return None

    category_raw_metrics = defaultdict(lambda: defaultdict(
        lambda: {"total_mentions": 0, "first_pos_sum": 0, "strong_recommend_count": 0, "total_ref_count": 0,
                 "mention_in_answers": 0}))
    total_brand_mentions_across_all = 0
    for item in data:
        category = item.get("category", "Uncategorized")
        answer = item.get("response", {}).get("answer", "")
        references = item.get("response", {}).get("references", [])
        if not answer: continue
        answer_metrics = analyze_single_answer(answer, references, BRAND_DICTIONARY)
        for brand, metrics in answer_metrics.items():
            if brand in CHINESE_BRANDS_WHITELIST:
                cat_brand_metrics = category_raw_metrics[category][brand]
                cat_brand_metrics["total_mentions"] += metrics["mention_count"]
                if metrics["first_pos"] != float('inf'): cat_brand_metrics["first_pos_sum"] += metrics["first_pos"]
                cat_brand_metrics["strong_recommend_count"] += metrics["is_strong"]
                cat_brand_metrics["total_ref_count"] += metrics["ref_count"]
                cat_brand_metrics["mention_in_answers"] += 1
                total_brand_mentions_across_all += metrics["mention_count"]

    final_data_for_df = []
    for category, brands_metrics in category_raw_metrics.items():
        if not brands_metrics: continue
        max_mentions = max(m["total_mentions"] for m in brands_metrics.values()) if brands_metrics else 0
        min_pos_avg = min(m["first_pos_sum"] / m["mention_in_answers"] for m in brands_metrics.values() if
                          m["mention_in_answers"] > 0) if any(
            m["mention_in_answers"] > 0 for m in brands_metrics.values()) else 0
        max_strong = max(m["strong_recommend_count"] for m in brands_metrics.values()) if brands_metrics else 0
        max_refs = max(m["total_ref_count"] for m in brands_metrics.values()) if brands_metrics else 0

        for brand, metrics in brands_metrics.items():
            scores = {}
            avg_pos = metrics["first_pos_sum"] / metrics["mention_in_answers"] if metrics[
                                                                                      "mention_in_answers"] > 0 else float(
                'inf')
            scores["visibility"] = (1 - normalize_score(avg_pos, min_pos_avg * 5, min_pos_avg) / 100) * WEIGHTS_V3[
                "visibility"]
            scores["mention_rate"] = normalize_score(metrics["total_mentions"], max_mentions) / 100 * WEIGHTS_V3[
                "mention_rate"]
            scores["ai_ranking"] = normalize_score(metrics["strong_recommend_count"], max_strong) / 100 * WEIGHTS_V3[
                "ai_ranking"]
            scores["ref_depth"] = normalize_score(metrics["total_ref_count"], max_refs) / 100 * WEIGHTS_V3["ref_depth"]
            mind_share_ratio = metrics[
                                   "total_mentions"] / total_brand_mentions_across_all if total_brand_mentions_across_all > 0 else 0
            scores["mind_share"] = mind_share_ratio * 100 * (WEIGHTS_V3["mind_share"] / 5)
            comp_score_avg = (scores["visibility"] + scores["mention_rate"] + scores["ai_ranking"]) / (
                        WEIGHTS_V3["visibility"] + WEIGHTS_V3["mention_rate"] + WEIGHTS_V3["ai_ranking"]) if (
                                                                                                                         WEIGHTS_V3[
                                                                                                                             "visibility"] +
                                                                                                                         WEIGHTS_V3[
                                                                                                                             "mention_rate"] +
                                                                                                                         WEIGHTS_V3[
                                                                                                                             "ai_ranking"]) > 0 else 0
            scores["competitiveness"] = comp_score_avg * WEIGHTS_V3["competitiveness"]
            total_score = sum(scores.values())

            final_data_for_df.append({
                "å“ç±»": category,
                "å“ç‰Œåç§°": brand,
                "å“ç‰ŒæŒ‡æ•°": total_score,
                "æ€»æåŠæ¬¡æ•°": metrics["total_mentions"],
                "å‡ºç°æ¬¡æ•°": metrics["mention_in_answers"],
                "æœ€åæ›´æ–°æ—¶é—´": time.strftime('%Y/%m/%d %H:%M:%S')
            })
    return pd.DataFrame(final_data_for_df)


# --- 4. Streamlit ç•Œé¢æ¸²æŸ“ ---
st.set_page_config(page_title="GenAIå“ç‰Œæ’è¡Œæ¦œ", layout="wide")

st.title("ğŸ¤– GenAIå¿ƒæ™ºå æœ‰ç‡ - å“ç‰Œæ’è¡Œæ¦œ")

# åŠ è½½æ•°æ®c
df = get_ranking_data()

if df is not None and not df.empty:
    # åˆ›å»ºç­›é€‰å™¨
    all_categories = ["å…¨éƒ¨å“ç±» (èšåˆ)"] + sorted(df["å“ç±»"].unique())
    selected_category = st.selectbox("**ç­›é€‰å“ç±»:**", all_categories)

    # æ ¹æ®ç­›é€‰ç»“æœå±•ç¤ºæ•°æ®
    if selected_category == "å…¨éƒ¨å“ç±» (èšåˆ)":
        st.header("å®¶ç”¨ç”µå™¨ - æ€»æ¦œå• (ç»¼åˆæ’å)")
        # è®¡ç®—æ€»æ¦œå•
        df_agg = df.groupby("å“ç‰Œåç§°").agg({
            "å“ç‰ŒæŒ‡æ•°": "sum",
            "æ€»æåŠæ¬¡æ•°": "sum",
            "å‡ºç°æ¬¡æ•°": "sum",
        }).reset_index()
        df_agg = df_agg.sort_values(by="å“ç‰ŒæŒ‡æ•°", ascending=False).reset_index(drop=True)
        df_agg.index = df_agg.index + 1
    else:
        st.header(f"å“ç±» - {selected_category}")
        df_agg = df[df["å“ç±»"] == selected_category].sort_values(by="å“ç‰ŒæŒ‡æ•°", ascending=False).reset_index(drop=True)
        df_agg.index = df_agg.index + 1
        df_agg = df_agg.drop(columns=["å“ç±»"])

    # ä½¿ç”¨st.dataframeæ¥å±•ç¤ºï¼Œå¹¶å¯ä»¥è¿›è¡Œä¸€äº›æ ·å¼é…ç½®
    st.dataframe(
        df_agg.style.format({"å“ç‰ŒæŒ‡æ•°": "{:.2f}"}).highlight_max(subset=["å“ç‰ŒæŒ‡æ•°"], color="lightgreen"),
        use_container_width=True
    )
else:
    st.error("æ•°æ®åŠ è½½å¤±è´¥ï¼Œè¯·ç¡®ä¿ 'results.json' æ–‡ä»¶å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®ã€‚")

